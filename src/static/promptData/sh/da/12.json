[[{"text":"Q. Why does $q$-Learning tend to over-estimate $q$-values?\nA. When the maximum $q$-value for the successor state is over-estimated, that over-estimate will propagate to the $q$-value update for the current state. (Under-estimates will only get propagated if they're still the maximum.)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[10]","startOffset":125,"endContainer":"/div[1]/p[10]","endOffset":270},{"type":"TextPositionSelector","start":1241,"end":1386},{"type":"TextQuoteSelector","exact":"This means if there’s a small positive error in the Q-value estimate, this overestimate will be transferred to the preceding state’s Q-values too","prefix":" value of the successor action. ","suffix":".\nThe best successor state value"}]}]},{"text":"Q. How does Double DQN differ from DQN?\nA. It uses the current network to select an action and a separate \"target network\" to evaluate the target for the loss function.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[26]","startOffset":0,"endContainer":"/div[1]/p[26]","endOffset":71},{"type":"TextPositionSelector","start":3787,"end":3858},{"type":"TextQuoteSelector","exact":"Double DQN uses the current network being trained for action selection.","prefix":"t{selected}})q(st+1​,aselected​)","suffix":" It uses a ‘target network’ for "}]}]},{"text":"Q. How is the \"target network\" for Double DQN usually derived?\nA. It's usually an old version of the network being trained, updated to the current network every ~10 episodes.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[26]","startOffset":126,"endContainer":"/div[1]/p[26]","endOffset":296},{"type":"TextPositionSelector","start":3913,"end":4083},{"type":"TextQuoteSelector","exact":"The target network is usually an old version of the network being trained. Typically they update the target network to match the current network roughly every 10 episodes","prefix":"ork’ for the action evaluation. ","suffix":", but it becomes out of date as "}]}]},{"text":"Q. When training a Double DQN in PyTorch, how should you initialize the target network from the main network so that updates to one don't affect the other?\nA. `a = copy.deepcopy(b)` (after importing `copy`)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/pre[1]/code[1]","startOffset":13,"endContainer":"/div[1]/pre[1]/code[1]","endOffset":41},{"type":"TextPositionSelector","start":5188,"end":5216},{"type":"TextQuoteSelector","exact":"target_Q = copy.deepcopy(Q)\n","prefix":"he current network:import copy\n\n","suffix":"\ndef update_target(target: nn.Mo"}]}]},{"text":"Q. In PyTorch, how can you update one network to match another?\nA. `dest.load_state_dict(source.state_dict())`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/pre[1]/code[1]","startOffset":104,"endContainer":"/div[1]/p[35]","endOffset":0},{"type":"TextPositionSelector","start":5279,"end":5323},{"type":"TextQuoteSelector","exact":"target.load_state_dict(network.state_dict())","prefix":"odule, network: nn.Module):\n    ","suffix":"\n\n\n3. Tips for TrainingThis is a"}]}]},{"text":"Q. How to normalize the inputs to a neural network?\nA. $x_{\\text{norm}} = \\frac{x-\\mu}{\\sigma}$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[61]","startOffset":0,"endContainer":"/div[1]/p[61]","endOffset":23},{"type":"TextPositionSelector","start":9846,"end":9869},{"type":"TextQuoteSelector","exact":"Mathematically this is:","prefix":"e square root of the variance). ","suffix":"@import url('https://cdnjs.cloud"}]}]},{"text":"Q. What problems do unnormalized inputs cause for training neural networks?\nA. e.g. input dimensions may have wildly different magnitudes (parameters will have to make huge changes), inputs may all have the same sign (input weights can't distinguish between bottom and top half of distribution)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[65]","startOffset":62,"endContainer":"/div[1]/p[65]","endOffset":86},{"type":"TextPositionSelector","start":10480,"end":10504},{"type":"TextQuoteSelector","exact":"These are outlined below","prefix":"roblem for statistical reasons. ","suffix":".One reason that unnormalized in"}]}]},{"text":"Q. When normalizing a neural network's input data, what invariant must you maintain between training episodes, and between training and testing?\nA. The normalization procedure must not change between these contexts—i.e. you must use the same values of $\\mu$ and $\\sigma$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[71]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[71]","endOffset":107},{"type":"TextPositionSelector","start":11606,"end":11713},{"type":"TextQuoteSelector","exact":"Note: the normalization procedure must not change between training episodes or between training and testing","prefix":"values of that input dimension.\n","suffix":". Changing this will change the "}]}]},{"text":"Q. What is \"frame-skipping\" in DQN?\nA. Choose an action, then repeat it for $k$ frames before making another decision.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[74]","startOffset":0,"endContainer":"/div[1]/p[74]","endOffset":220},{"type":"TextPositionSelector","start":11847,"end":12067},{"type":"TextQuoteSelector","exact":"A useful trick used in the original DQN paper for training in many discrete-time environments is to skip @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')kkk﻿ frames between decision-making","prefix":"sly learned.\n\n3.3 Frame-Skipping","suffix":". Here, your agent selects an ac"}]}]},{"text":"Q. What are some benefits of frame-skipping in DQN?\nA. e.g. it amplifies differences between actions' $q$-values (so you can discriminate faster); it reduces the number propagation steps between high-reward goal states and initial states","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[75]","startOffset":0,"endContainer":"/div[1]/p[75]","endOffset":25},{"type":"TextPositionSelector","start":12360,"end":12385},{"type":"TextQuoteSelector","exact":"This has several benefits","prefix":"t is queried for another action.","suffix":". Firstly, it makes each action "}]}]},{"text":"Q. What does the DQN update frequency specify?\nA. The frequency of updates to the neural network; i.e. the reciprocal of the number of time steps of experience between updates.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[82]","startOffset":0,"endContainer":"/div[1]/p[82]","endOffset":29},{"type":"TextPositionSelector","start":14028,"end":14057},{"type":"TextQuoteSelector","exact":"What’s the update frequency? ","prefix":"hey blink.\n\n3.4 Update Frequency","suffix":"The reciprocal of the number of "}]}]},{"text":"Q. When might it be better to choose a lower or higher update frequency when training a DQN?\nA. A higher update frequency might be better if environment timesteps are costly; lower frequencies might be better if they're quick, since then you can cover more ground.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[87]","startOffset":0,"endContainer":"/div[1]/p[87]","endOffset":125},{"type":"TextPositionSelector","start":15135,"end":15260},{"type":"TextQuoteSelector","exact":"So you want to choose an update frequency based on how easy it is to generate new data versus how fast it is to make updates.","prefix":"e space is the limiting factor. ","suffix":"How ‘lightweight’ the environmen"}]}]},{"text":"Q. How to invert a Boolean tensor in PyTorch?\nA. `~t`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ul[4]/li[1]/details[1]/p[2]","startOffset":0,"endContainer":"/div[1]/ul[4]/li[1]/details[1]/p[2]","endOffset":26},{"type":"TextPositionSelector","start":18831,"end":18857},{"type":"TextQuoteSelector","exact":"To invert a boolean tensor","prefix":"rue is cast to 1 and False to 0.","suffix":", you can use ~.E.g.>>> is_done_"}]}]}]]