[[{"text":"Q. Why does $q$-Learning tend to over-estimate $q$-values?\nA. When the maximum $q$-value for the successor state is over-estimated, that over-estimate will propagate to the $q$-value update for the current state. (Under-estimates will only get propagated if they're still the maximum.)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[10]","startOffset":125,"endContainer":"/div[1]/p[10]","endOffset":270},{"type":"TextPositionSelector","start":1241,"end":1386},{"type":"TextQuoteSelector","exact":"This means if there’s a small positive error in the Q-value estimate, this overestimate will be transferred to the preceding state’s Q-values too","prefix":" value of the successor action. ","suffix":".\nThe best successor state value"}]}]},{"text":"Q. How does Double DQN differ from DQN?\nA. The target used to update the network towards is different. Double DQN uses the current network to select the successor action and a separate \"target network\" to evaluate it.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[26]","startOffset":0,"endContainer":"/div[1]/p[26]","endOffset":71},{"type":"TextPositionSelector","start":3787,"end":3858},{"type":"TextQuoteSelector","exact":"Double DQN uses the current network being trained for action selection.","prefix":"t{selected}})q(st+1​,aselected​)","suffix":" It uses a ‘target network’ for "}]}]},{"text":"Q. How is the \"target network\" for Double DQN usually derived?\nA. It's usually an old version of the network being trained, updated to the current network every ~10 episodes.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[26]","startOffset":126,"endContainer":"/div[1]/p[26]","endOffset":296},{"type":"TextPositionSelector","start":3913,"end":4083},{"type":"TextQuoteSelector","exact":"The target network is usually an old version of the network being trained. Typically they update the target network to match the current network roughly every 10 episodes","prefix":"ork’ for the action evaluation. ","suffix":", but it becomes out of date as "}]}]},{"text":"Q. When training a Double DQN in PyTorch, how should you initialize the target network from the main network so that updates to one don't affect the other?\nA. `a = copy.deepcopy(b)` (after importing `copy`)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/pre[1]/code[1]","startOffset":13,"endContainer":"/div[1]/pre[1]/code[1]","endOffset":41},{"type":"TextPositionSelector","start":5188,"end":5216},{"type":"TextQuoteSelector","exact":"target_Q = copy.deepcopy(Q)\n","prefix":"he current network:import copy\n\n","suffix":"\ndef update_target(target: nn.Mo"}]}]},{"text":"Q. In PyTorch, how can you update one network to match another?\nA. `dest.load_state_dict(source.state_dict())`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/pre[1]/code[1]","startOffset":104,"endContainer":"/div[1]/p[35]","endOffset":0},{"type":"TextPositionSelector","start":5279,"end":5323},{"type":"TextQuoteSelector","exact":"target.load_state_dict(network.state_dict())","prefix":"odule, network: nn.Module):\n    ","suffix":"\n\n\n3. Tips for TrainingThis is a"}]}]},{"text":"Q. How does one normalize a random variable $x$ with mean $\\mu$ and variance $\\sigma$?\nA. $x_{\\text{norm}} = \\frac{x-\\mu}{\\sigma}$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[61]","startOffset":0,"endContainer":"/div[1]/p[61]","endOffset":23},{"type":"TextPositionSelector","start":9846,"end":9869},{"type":"TextQuoteSelector","exact":"Mathematically this is:","prefix":"e square root of the variance). ","suffix":"@import url('https://cdnjs.cloud"}]}]},{"text":"Q. When normalizing a neural network's input data, what must not change between training and testing?\nA. The normalization procedure must not change between these contexts—i.e. you must use the same values of $\\mu$ and $\\sigma$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[71]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[71]","endOffset":107},{"type":"TextPositionSelector","start":11606,"end":11713},{"type":"TextQuoteSelector","exact":"Note: the normalization procedure must not change between training episodes or between training and testing","prefix":"values of that input dimension.\n","suffix":". Changing this will change the "}]}]},{"text":"Q. What is \"frame-skipping\" in DQN?\nA. Choose an action, then repeat it for $k$ frames before making another decision.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[74]","startOffset":0,"endContainer":"/div[1]/p[74]","endOffset":220},{"type":"TextPositionSelector","start":11847,"end":12067},{"type":"TextQuoteSelector","exact":"A useful trick used in the original DQN paper for training in many discrete-time environments is to skip @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')kkk﻿ frames between decision-making","prefix":"sly learned.\n\n3.3 Frame-Skipping","suffix":". Here, your agent selects an ac"}]}]},{"text":"Q. What are some benefits of frame-skipping in DQN?\nA. e.g. it amplifies differences between actions' $q$-values (so you can discriminate faster); it reduces the number of steps between high-reward goal states and initial states for information to propagate back.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[75]","startOffset":0,"endContainer":"/div[1]/p[75]","endOffset":25},{"type":"TextPositionSelector","start":12360,"end":12385},{"type":"TextQuoteSelector","exact":"This has several benefits","prefix":"t is queried for another action.","suffix":". Firstly, it makes each action "}]}]},{"text":"Q. In reinforcement learning using neural networks, what is the 'update frequency'?\nA. The frequency of gradient updates to the neural network.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[82]","startOffset":0,"endContainer":"/div[1]/p[82]","endOffset":29},{"type":"TextPositionSelector","start":14028,"end":14057},{"type":"TextQuoteSelector","exact":"What’s the update frequency? ","prefix":"hey blink.\n\n3.4 Update Frequency","suffix":"The reciprocal of the number of "}]}]},{"text":"Q. When might it be better to choose a lower or higher update frequency when training a DQN?\nA. A higher update frequency might be better if environment timesteps are costly; lower frequencies might be better if they're quick, since then you can cover more ground.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[87]","startOffset":0,"endContainer":"/div[1]/p[87]","endOffset":125},{"type":"TextPositionSelector","start":15135,"end":15260},{"type":"TextQuoteSelector","exact":"So you want to choose an update frequency based on how easy it is to generate new data versus how fast it is to make updates.","prefix":"e space is the limiting factor. ","suffix":"How ‘lightweight’ the environmen"}]}]},{"text":"Q. How to invert a Boolean tensor in PyTorch?\nA. `~t`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/ul[4]/li[1]/details[1]/p[2]","startOffset":0,"endContainer":"/div[1]/ul[4]/li[1]/details[1]/p[2]","endOffset":26},{"type":"TextPositionSelector","start":18831,"end":18857},{"type":"TextQuoteSelector","exact":"To invert a boolean tensor","prefix":"rue is cast to 1 and False to 0.","suffix":", you can use ~.E.g.>>> is_done_"}]}]},{"text":"Q. When training a neural network, what is the batch size?\nA. The number of datapoints used to calculate the loss function for one update step.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[105]","startOffset":0,"endContainer":"/div[1]/p[105]","endOffset":111},{"type":"TextPositionSelector","start":20613,"end":20724},{"type":"TextQuoteSelector","exact":"By ‘batch size’, we mean the number of state transitions we use to calculate the loss used in each update step.","prefix":"rm a new tensor.\n\n3.7 Batch size","suffix":" We discussed this briefly befor"}]}]}]]