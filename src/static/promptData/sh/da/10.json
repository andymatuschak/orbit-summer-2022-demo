[[{"text":"Q. What distinguishes *model-based* and *model-free* RL agents?\nA. Model-based agents use a model of the environment. Model-free agents don't.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[13]","startOffset":0,"endContainer":"/div[1]/p[13]","endOffset":86},{"type":"TextPositionSelector","start":1966,"end":2052},{"type":"TextQuoteSelector","exact":"Agents that use a model are called model-based agents, while model-free agents do not.","prefix":"erience in the real environment\n","suffix":"\nUp to this point, we’ve been us"}]}]},{"text":"Q. What are the key advantages of model-based RL?\nA. You can make decisions using simulations of the future, so you need fewer real samples from the environment to learn effectively. This is useful when gathering experience is expensive or unsafe.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[25]","startOffset":0,"endContainer":"/div[1]/p[25]","endOffset":90},{"type":"TextPositionSelector","start":3136,"end":3226},{"type":"TextQuoteSelector","exact":"The key benefit of using a model is that you can make decisions using simulated experience","prefix":". \n\n2. Should you use a model?\n\n","suffix":", rather than relying on actual "}]}]},{"text":"Q. Give an example of a situation where a model-free RL approach is preferable.\nA. e.g. autonomous driving (hard to predict other road users); investing (hard to predict other investors); online ads (hard to predict user behavior)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[31]","startOffset":0,"endContainer":"/div[1]/p[31]","endOffset":58},{"type":"TextPositionSelector","start":4382,"end":4440},{"type":"TextQuoteSelector","exact":"And it’s not just autonomous driving which has this issue:","prefix":"nd as-of-yet unsolved) problem.\n","suffix":"Robotics - If we train a robot t"}]}]},{"text":"Q. What's the more descriptive name for $q$-functions?\nA. Action-value functions","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[48]/span[1]","startOffset":0,"endContainer":"/div[1]/p[48]/span[1]","endOffset":10},{"type":"TextPositionSelector","start":7998,"end":8008},{"type":"TextQuoteSelector","exact":"Q-function","prefix":"l DefinitionWe can describe the ","suffix":" with the following equation, wh"}]}]},{"text":"Q. Define the optimal action-value function, $q_*(s,a)$, in terms of $q_{\\pi}$.\nA. $q_*(s,a) = \\max_{\\pi} q_{\\pi}(s,a)$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[68]","startOffset":212,"endContainer":"/div[1]/p[68]/span[2]/span[3]","endOffset":1},{"type":"TextPositionSelector","start":12584,"end":12719},{"type":"TextQuoteSelector","exact":"optimal action-value function @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')q∗(s,a)q_*(s, a)q∗​(s,a)﻿","prefix":"s)\\pi^*(s)π∗(s)﻿. We define the ","suffix":" as follows:@import url('https:/"}]}]},{"text":"Q. Give the optimal state-value function, $v_*(s)$, in terms of $q_*$.\nA. $v_*(s) = \\max_{a'} q_*(s,a')$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[72]","startOffset":0,"endContainer":"/div[1]/p[72]","endOffset":24},{"type":"TextPositionSelector","start":14135,"end":14159},{"type":"TextQuoteSelector","exact":"Mathematically, this is:","prefix":" as taking the optimal policy).\n","suffix":"@import url('https://cdnjs.cloud"}]}]},{"text":"Q. Give the Bellman equation for $q_*$, in recursive form.\nA. $q_*(s,a) = \\mathbb{E}_{\\pi^*}[r_{t+1} + \\gamma \\max_{a'} q_*(s_{t+1}, a') | s_t = s, a_t = a]$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[75]","startOffset":417,"endContainer":"/div[1]/p[75]","endOffset":473},{"type":"TextPositionSelector","start":14869,"end":14925},{"type":"TextQuoteSelector","exact":"The Bellman Optimality Equation can therefore be written","prefix":"/0.13.2/katex.min.css')s′s's′﻿. ","suffix":":@import url('https://cdnjs.clou"}]}]},{"text":"Q. How does Q-learning avoid TD Learning's need for a model?\nA. By estimating the action-value function (instead of the state-value function), we can choose a greedy action without needing to know what state it will produce.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[83]","startOffset":0,"endContainer":"/div[1]/p[83]","endOffset":134},{"type":"TextPositionSelector","start":15727,"end":15861},{"type":"TextQuoteSelector","exact":"Q-learning allows us to do model-free control, by removing the reliance on doing a 1-step look-ahead to pick the best action greedily.","prefix":"lled Q-learning.\n\n\n5. Q-learning","suffix":" It does this by allowing us to "}]}]},{"text":"Q. What is the Q-learning update equation?\nA. $q(s_t, a_t) \\leftarrow (1 - \\alpha)q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma \\max_{a'} q(s_{t+1}, a'))$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[89]","startOffset":43,"endContainer":"/div[1]/p[89]","endOffset":165},{"type":"TextPositionSelector","start":16499,"end":16621},{"type":"TextQuoteSelector","exact":"we can update the @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')qqq﻿-function as follows","prefix":" 1-step fashion as TD Learning, ","suffix":":@import url('https://cdnjs.clou"}]}]},{"text":"Q. When $q$-learning, how do we implement $\\pi_{\\text{greedy}}$ mathematically?\nA. $\\pi_{\\text{greedy}}(s) = \\argmax_a \\hat{q}(s, a)$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[97]","startOffset":19,"endContainer":"/div[1]/p[97]","endOffset":185},{"type":"TextPositionSelector","start":21137,"end":21303},{"type":"TextQuoteSelector","exact":"state the greedy action selection using an approximate @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')qqq﻿-function explicitly below:","prefix":"ion SelectionTo be thorough, we ","suffix":"@import url('https://cdnjs.cloud"}]}]},{"text":"Q. What is a model in reinforcement learning?\nA. A predictor of the state transition and reward resulting from taking an action in the environment from the current state.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/blockquote[1]","startOffset":0,"endContainer":"/div[1]/blockquote[1]","endOffset":109},{"type":"TextPositionSelector","start":1107,"end":1216},{"type":"TextQuoteSelector","exact":"Definition: A model is a predictor of the state transition and reward resulting from actions the agent takes.","prefix":"TeX/0.13.2/katex.min.css')∗^*∗﻿\n","suffix":"\nThis predictor can either be di"}]}]},{"text":"Q. In words, explain what the 'action-value function' is.\nA. It is a function that outputs the value (expected future return) of being in state $s$ and taking action $a$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[44]","startOffset":194,"endContainer":"/div[1]/p[44]","endOffset":493},{"type":"TextPositionSelector","start":7211,"end":7510},{"type":"TextQuoteSelector","exact":"This is a function of both the state we’re in and the action we take in it. It gives the value if we’re in state @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')sss﻿ and take action @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa﻿.","prefix":".min.css')q(s,a)q(s, a)q(s,a)﻿. ","suffix":" We’ve seen these very briefly b"}]}]},{"text":"Q. In words, how do we use the action-value function to perform greedy action selection?\nA. Pick the action from the current state with the highest action-value.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[96]","startOffset":0,"endContainer":"/div[1]/p[96]","endOffset":426},{"type":"TextPositionSelector","start":18821,"end":19247},{"type":"TextQuoteSelector","exact":"In English, the @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')arg max⁡aq^(s,a)\\argmax_a\\hat{q}(s, a)argmaxa​q^​(s,a)﻿ simply returns the action @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')aaa﻿ that maximizes the approximate action-value function @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')q^(s,a)\\hat{q}(s, a)q^​(s,a)﻿.","prefix":", a)πgreedy​(s)=aargmax​q^​(s,a)","suffix":"\n\n5.3 Q-Learning algorithmBelow "}]}]}]]