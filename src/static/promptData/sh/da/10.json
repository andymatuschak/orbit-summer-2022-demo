[[{"text":"Q. What distinguishes *model-based* and *model-free* RL agents?\nA. Model-based agents have a predictor of the state transition and reward for a given action. Model-free agents don't.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[13]","startOffset":0,"endContainer":"/div[1]/p[13]","endOffset":86},{"type":"TextPositionSelector","start":1966,"end":2052},{"type":"TextQuoteSelector","exact":"Agents that use a model are called model-based agents, while model-free agents do not.","prefix":"erience in the real environment\n","suffix":"\nUp to this point, we’ve been us"}]}]},{"text":"Q. When would 1-step greedy look-ahead be a *model-based* method for policy improvement?\nA. If you have an estimate of the value function (rather than the action-value function), you'll need a predictor of the state transition function to choose an optimal action.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[15]","startOffset":18,"endContainer":"/div[1]/p[15]","endOffset":114},{"type":"TextPositionSelector","start":2071,"end":2167},{"type":"TextQuoteSelector","exact":"we’ve been using a model to perform a 1-step greedy look-ahead to do the policy improvement step","prefix":"gents do not.\nUp to this point, ","suffix":". This is therefore model-based "}]}]},{"text":"Q. What are the key advantages of model-based RL?\nA. You can make decisions using simulations of the future, so you need fewer real samples from the environment to learn effectively.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[25]","startOffset":0,"endContainer":"/div[1]/p[25]","endOffset":90},{"type":"TextPositionSelector","start":3136,"end":3226},{"type":"TextQuoteSelector","exact":"The key benefit of using a model is that you can make decisions using simulated experience","prefix":". \n\n2. Should you use a model?\n\n","suffix":", rather than relying on actual "}]}]},{"text":"Q. Give an example of a situation where a model-free RL approach is preferable.\nA. e.g. autonomous driving (hard to predict other road users); investing (hard to predict other investors); online ads (hard to predict user behavior)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[31]","startOffset":0,"endContainer":"/div[1]/p[31]","endOffset":58},{"type":"TextPositionSelector","start":4382,"end":4440},{"type":"TextQuoteSelector","exact":"And it’s not just autonomous driving which has this issue:","prefix":"nd as-of-yet unsolved) problem.\n","suffix":"Robotics - If we train a robot t"}]}]},{"text":"Q. Give the general form of the action-value function $q(s,a)$ in terms of $G_t$ and a policy $\\pi$.\nA. $q(s,a) = \\mathbb{E}_{\\pi}[G_t|s_t=s, a_t=a]$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[48]","startOffset":20,"endContainer":"/div[1]/p[48]","endOffset":30},{"type":"TextPositionSelector","start":7998,"end":8008},{"type":"TextQuoteSelector","exact":"Q-function","prefix":"l DefinitionWe can describe the ","suffix":" with the following equation, wh"}]}]},{"text":"Q. What's the more descriptive name for $q$-functions?\nA. Action-value functions","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[48]/span[1]","startOffset":0,"endContainer":"/div[1]/p[48]/span[1]","endOffset":10},{"type":"TextPositionSelector","start":7998,"end":8008},{"type":"TextQuoteSelector","exact":"Q-function","prefix":"l DefinitionWe can describe the ","suffix":" with the following equation, wh"}]}]},{"text":"Q. Give the Bellman equation for the action-value function, $q_{\\pi}(s,a)$, in terms of $v_\\pi$.\nA. $q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[r_{t+1} + \\gamma v_{\\pi}(s_{t+1})|s_t = s, a_t = a]$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[66]","startOffset":0,"endContainer":"/div[1]/p[66]","endOffset":150},{"type":"TextPositionSelector","start":11987,"end":12137},{"type":"TextQuoteSelector","exact":"Lucky for us, there’s a very similar equivalent for the @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')qqq﻿-function:","prefix":"s)=Eπ​[rt+1​+γvπ​(st+1​)∣st​=s]\n","suffix":"@import url('https://cdnjs.cloud"}]}]},{"text":"Q. Define the optimal action-value function, $q_*(s,a)$, in terms of $q_{\\pi}$.\nA. $q_*(s,a) = \\max_{\\pi} q_{\\pi}(s,a)$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[68]","startOffset":212,"endContainer":"/div[1]/p[68]/span[2]/span[3]","endOffset":1},{"type":"TextPositionSelector","start":12584,"end":12719},{"type":"TextQuoteSelector","exact":"optimal action-value function @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')q∗(s,a)q_*(s, a)q∗​(s,a)﻿","prefix":"s)\\pi^*(s)π∗(s)﻿. We define the ","suffix":" as follows:@import url('https:/"}]}]},{"text":"Q. Give the optimal state-value function, $v_*(s)$, in terms of $q_*$.\nA. $v_*(s) = \\max_{a'} q_*(s,a')$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[72]","startOffset":0,"endContainer":"/div[1]/p[72]","endOffset":24},{"type":"TextPositionSelector","start":14135,"end":14159},{"type":"TextQuoteSelector","exact":"Mathematically, this is:","prefix":" as taking the optimal policy).\n","suffix":"@import url('https://cdnjs.cloud"}]}]},{"text":"Q. Give the Bellman equation for $q_*$, in recursive form.\nA. $q_*(s,a) = \\mathbb{E}_{\\pi^*}[r_{t+1} + \\gamma \\max_{a'} q_*(s_{t+1}, a') | s_t = s, a_t = a]$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[75]","startOffset":417,"endContainer":"/div[1]/p[75]","endOffset":473},{"type":"TextPositionSelector","start":14869,"end":14925},{"type":"TextQuoteSelector","exact":"The Bellman Optimality Equation can therefore be written","prefix":"/0.13.2/katex.min.css')s′s's′﻿. ","suffix":":@import url('https://cdnjs.clou"}]}]},{"text":"Q. How does Q-learning avoid TD Learning's need for a model?\nA. By estimating the action-value function (instead of the state-value function), we can choose a greedy action without needing to know what state it will produce.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[83]","startOffset":0,"endContainer":"/div[1]/p[83]","endOffset":134},{"type":"TextPositionSelector","start":15727,"end":15861},{"type":"TextQuoteSelector","exact":"Q-learning allows us to do model-free control, by removing the reliance on doing a 1-step look-ahead to pick the best action greedily.","prefix":"lled Q-learning.\n\n\n5. Q-learning","suffix":" It does this by allowing us to "}]}]},{"text":"Q. What is the Q-learning update equation?\nA. $q(s_t, a_t) \\leftarrow (1 - \\alpha)q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma \\max_{a'} q(s_{t+1}, a'))$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[89]","startOffset":43,"endContainer":"/div[1]/p[89]","endOffset":165},{"type":"TextPositionSelector","start":16499,"end":16621},{"type":"TextQuoteSelector","exact":"we can update the @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')qqq﻿-function as follows","prefix":" 1-step fashion as TD Learning, ","suffix":":@import url('https://cdnjs.clou"}]}]},{"text":"Q. When $q$-learning, how do we implement $\\pi_{\\text{greedy}}$?\nA. $\\pi_{\\text{greedy}}(s) = \\argmax_a \\hat{q}(s, a)$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[97]","startOffset":19,"endContainer":"/div[1]/p[97]","endOffset":185},{"type":"TextPositionSelector","start":21137,"end":21303},{"type":"TextQuoteSelector","exact":"state the greedy action selection using an approximate @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')qqq﻿-function explicitly below:","prefix":"ion SelectionTo be thorough, we ","suffix":"@import url('https://cdnjs.cloud"}]}]}]]