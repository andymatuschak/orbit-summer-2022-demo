[[{"text":"Q. What is PyTorch's specialized data structure for arrays and lists?\nA. Tensors (`torch.tensor()`)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[11]","startOffset":0,"endContainer":"/div[1]/p[11]/code[1]","endOffset":5},{"type":"TextPositionSelector","start":1455,"end":1509},{"type":"TextQuoteSelector","exact":"Tensors are a specialized data structure used in torch","prefix":"faulty code.\n\n\nTensors and NumPy","suffix":". In PyTorch, tensors are used f"}]}]},{"text":"Q. What is PyTorch's default `dtype` for neural networks?\nA. `torch.float32`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[30]","startOffset":102,"endContainer":"/div[1]/p[30]","endOffset":160},{"type":"TextPositionSelector","start":3854,"end":3912},{"type":"TextQuoteSelector","exact":"is the default dtype for neural networks","prefix":", 2, 3], dtype=torch.float32) - ","suffix":", so it‚Äôs typically a safe bet c"}]}]},{"text":"Q. How to ensure that a PyTorch tensor of integers has a floating-point `dtype`?\nA. Either `torch.tensor([1,2], dtype=torch.float32)` or call `.float()` on the tensor","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[30]/code[2]","startOffset":0,"endContainer":"/div[1]/p[30]/code[2]","endOffset":44},{"type":"TextPositionSelector","start":3807,"end":3851},{"type":"TextQuoteSelector","exact":"torch.tensor([1, 2, 3], dtype=torch.float32)","prefix":"e dtypes of tensors as follows: ","suffix":" - the torch.float32 is the defa"}]}]},{"text":"Q. How to combine several PyTorch tensors into a single higher-dimensional tensor?\nA. `torch.stack(t1, t2)`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[37]","startOffset":39,"endContainer":"/div[1]/p[37]","endOffset":142},{"type":"TextPositionSelector","start":4608,"end":4711},{"type":"TextQuoteSelector","exact":"What it should say is: use torch.stack() instead! since  torch.stack(list_of_tensors) works beautifully","prefix":"ror isn‚Äôt particularly helpful. ","suffix":"!You‚Äôll thank me later.\nExample "}]}]},{"text":"Q. What are the required arguments to `torch.nn.Linear`?\nA. `in_features` and `out_features`: the input count and neuron count, respectively.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/h3[5]/code[1]/strong[1]","startOffset":0,"endContainer":"/div[1]/h3[5]/code[1]/strong[1]","endOffset":36},{"type":"TextPositionSelector","start":8154,"end":8190},{"type":"TextQuoteSelector","exact":"nn.Linear(in_features, out_features)","prefix":"m, so what are we waiting for???","suffix":"The linear layer is a set of sta"}]}]},{"text":"Q. What activation function does `torch.nn.Linear` automatically implement?\nA. None. Activation functions are separate in `torch`.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[51]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[51]/strong[1]","endOffset":30},{"type":"TextPositionSelector","start":8273,"end":8303},{"type":"TextQuoteSelector","exact":"without an activation function","prefix":"hted sum of inputs + bias term) ","suffix":", using its stored weights and b"}]}]},{"text":"Q. What PyTorch function creates a linear layer of neurons?\nA. `torch.nn.Linear`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/h3[5]/code[1]/strong[1]/span[1]","startOffset":0,"endContainer":"/div[1]/h3[5]/code[1]/strong[1]/span[1]","endOffset":36},{"type":"TextPositionSelector","start":8154,"end":8190},{"type":"TextQuoteSelector","exact":"nn.Linear(in_features, out_features)","prefix":"m, so what are we waiting for???","suffix":"The linear layer is a set of sta"}]}]},{"text":"Q. In PyTorch, how do you specify the activation function for neural network layers?\nA. Activation functions are treated as separate layers; e.g. you create a `nn.Linear` layer and pass its output to a `nn.ReLU` layer.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[62]/code[1]","startOffset":0,"endContainer":"/div[1]/p[62]","endOffset":108},{"type":"TextPositionSelector","start":10336,"end":10444},{"type":"TextQuoteSelector","exact":"torch views activation functions as ‚Äòlayers‚Äô - they are added after linear layers to introduce non-linearity","prefix":"\\text{max}(0, x)ReLU(x)=max(0,x)","suffix":", helping neural networks approx"}]}]},{"text":"Q. In PyTorch, what's the easiest way to combine multiple layers into a network?\nA. `torch.nn.Sequential(l1, l2, ...)`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[67]/code[1]","startOffset":0,"endContainer":"/div[1]/p[67]/code[1]","endOffset":13},{"type":"TextPositionSelector","start":12474,"end":12487},{"type":"TextQuoteSelector","exact":"nn.Sequential","prefix":"rch.Size([3, 20])\n\nnn.Sequential","suffix":"¬†is an ordered container of neur"}]}]},{"text":"Q. Given a PyTorch neural network constructed via `n = nn.Sequential(...)`, how would you compute its outputs for an input tensor `x`?\nA. `output = n(x)`","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/pre[5]/code[1]","startOffset":483,"endContainer":"/div[1]/pre[5]/code[1]","endOffset":508},{"type":"TextPositionSelector","start":13338,"end":13363},{"type":"TextQuoteSelector","exact":"seq_modules(input_tensor)","prefix":" 3 datapoints\n\nnetwork_output = ","suffix":"\n\nprint(network_output)\nThis giv"}]}]},{"text":"Q. What distinguishes supervised learning from other ML methods?\nA. Supervised learning models are trained with *labeled data*: they find a function which approximates known outputs, given known inputs.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[102]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[102]/strong[1]","endOffset":19},{"type":"TextPositionSelector","start":16725,"end":16744},{"type":"TextQuoteSelector","exact":"supervised learning","prefix":"on from images is an example of ","suffix":". We have labelled data and we t"}]}]},{"text":"Q. What's meant by \"labeled data\" in ML?\nA. A set of inputs along with the true outputs. (e.g. images of digits are inputs with the digit they show as labels)","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[114]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[114]/strong[1]","endOffset":8},{"type":"TextPositionSelector","start":18142,"end":18150},{"type":"TextQuoteSelector","exact":"labelled","prefix":"a system that can 'learn' from '","suffix":"' data (images and then the labe"}]}]},{"text":"Q. In ML, what does the loss function specify?\nA. A model's performance (better models have lower losses).","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[118]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[118]/strong[1]","endOffset":13},{"type":"TextPositionSelector","start":18322,"end":18335},{"type":"TextQuoteSelector","exact":"loss function","prefix":" the broadest terms possible, a ","suffix":" is how we measure how well a mo"}]}]},{"text":"Q. Given $n$ data points with true output $\\hat Y_i$ and model output $Y_i$, mathematically define the mean squared error.\nA. $\\text{MSE}=\\frac{1}{n}\\sum_{i=1}^n(Y_i - \\hat Y_i)^2$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[130]","startOffset":0,"endContainer":"/div[1]/p[130]","endOffset":507},{"type":"TextPositionSelector","start":19750,"end":20257},{"type":"TextQuoteSelector","exact":"Mathematically this is defined below, where¬†@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ùëåùëñùëå_ùëñYi‚ÄãÔªø¬†is the neural network output for training example @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')iiiÔªø, and¬†@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')Y^i \\hat{Y}_iY^i‚ÄãÔªø is the true value for the @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')ithi^{th}ithÔªø data-point","prefix":"atapoints and output dimensions.","suffix":".@import url('https://cdnjs.clou"}]}]},{"text":"Q. Give a high-level verbal description of how gradient descent works.\nA. At each step, compute the gradient of the loss function with respect to the weights; update the weights in the direction of decreasing loss function values.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[141]","startOffset":0,"endContainer":"/div[1]/p[141]","endOffset":254},{"type":"TextPositionSelector","start":22284,"end":22538},{"type":"TextQuoteSelector","exact":"In this algorithm, at each step, we compute the gradient of our loss function with respect to the weight. We then update the weight in the direction that the gradient suggests will decrease the loss function. In this way, we 'optimize' our loss function.","prefix":" updates (steps) to the weights.","suffix":"You can think of this as walking"}]}]},{"text":"Q. What does the learning rate determine in gradient descent?\nA. How much we change our weights based on the gradient in each step.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[145]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[145]/strong[1]","endOffset":13},{"type":"TextPositionSelector","start":22865,"end":22878},{"type":"TextQuoteSelector","exact":"learning rate","prefix":"step. This is controlled by the¬†","suffix":".This is fraught with danger.  I"}]}]},{"text":"Q. What problems can occur if your gradient descent learning rate is poorly chosen?\nA. e.g. If it's too low, the network will converge very slowly and may be unable to escape local minimums. If it's too large, the weights might overshoot the optimal values.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[146]","startOffset":0,"endContainer":"/div[1]/p[146]","endOffset":30},{"type":"TextPositionSelector","start":22879,"end":22909},{"type":"TextQuoteSelector","exact":"This is fraught with danger.  ","prefix":"controlled by the¬†learning rate.","suffix":"If we our learning rate is too l"}]}]},{"text":"Q. In words, describe what the Mean-Squared Error (MSE) loss is.\nA. The error is the difference between the prediction and the target. The MSE is the mean of the squared error.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[129]","startOffset":0,"endContainer":"/div[1]/p[129]","endOffset":103},{"type":"TextPositionSelector","start":19647,"end":19750},{"type":"TextQuoteSelector","exact":"It then squares this 'error'. Then finally it takes the mean over all datapoints and output dimensions.","prefix":"e and the neural network output.","suffix":"Mathematically this is defined b"}]}]},{"text":"Q. What is 'gradient descent'?\nA. An algorithm that updates the parameters of a predictor to minimize a loss function.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[139]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[139]/strong[1]","endOffset":16},{"type":"TextPositionSelector","start":22176,"end":22192},{"type":"TextQuoteSelector","exact":"gradient descent","prefix":"hts, we use an algorithm called¬†","suffix":".This is an¬†iterative¬†algorithm,"}]}]}]]
