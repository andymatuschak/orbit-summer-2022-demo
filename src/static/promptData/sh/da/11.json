[[{"text":"Q. What does \"DQN\" stand for in RL?\nA. Deep Q-Networks","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[7]/strong[1]","startOffset":0,"endContainer":"/div[1]/p[7]/strong[1]","endOffset":15},{"type":"TextPositionSelector","start":1146,"end":1161},{"type":"TextQuoteSelector","exact":"Deep Q-Networks","prefix":"king at a legendary algorithm - ","suffix":".In 2013, DeepMind released a la"}]}]},{"text":"Q. How are states and actions modeled in DQN?\nA. The state is the input to the network; the output layer has 1 dimension for each possible action, representing the Q-value for that action, given the input state.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[19]","startOffset":0,"endContainer":"/div[1]/p[19]","endOffset":108},{"type":"TextPositionSelector","start":2923,"end":3031},{"type":"TextQuoteSelector","exact":"A better alternative is to take only the state as input and have 1 output dimension for each possible action","prefix":"ecially for large action spaces.","suffix":". Then each output dimension cor"}]}]},{"text":"Q. In DQN, why is the action not provided as input to the network?\nA. Choosing the greedy action would require evaluating the network once for every possible action—expensive!","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[18]","startOffset":85,"endContainer":"/div[1]/p[18]","endOffset":176},{"type":"TextPositionSelector","start":2519,"end":2610},{"type":"TextQuoteSelector","exact":"need to know the Q-values associated with every possible action to pick the maximum Q-value","prefix":"tion greedily in Q-learning, we ","suffix":". To do this with the above arch"}]}]},{"text":"Q. In DQN, what is the target value for the loss function?\nA. $r_{t+1} + \\gamma \\max_{a'} q(s_{t+1}, a')$","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[27]","startOffset":277,"endContainer":"/div[1]/p[28]","endOffset":0},{"type":"TextPositionSelector","start":5092,"end":5303},{"type":"TextQuoteSelector","exact":"the target we’re training it to predict is @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')rt+1+γmax⁡a′q(st+1,a′)r_{t+1} + \\gamma \\max_{a'}q(s_{t+1}, a')rt+1​+γmaxa′​q(st+1​,a′)﻿.","prefix":"t)q(s_t, a_t)q(st​,at​)﻿, while ","suffix":"This target is the equivalent of"}]}]},{"text":"Q. What function does a DQN approximate?\nA. The $q$-function, $q(s,a)$.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[19]","startOffset":110,"endContainer":"/div[1]/p[20]","endOffset":0},{"type":"TextPositionSelector","start":3033,"end":3103},{"type":"TextQuoteSelector","exact":"Then each output dimension corresponds to the Q-value for that action.","prefix":"nsion for each possible action. ","suffix":"\nInstead of using state-action p"}]}]},{"text":"Q. How does the goal implemented by $q$-Learning differ from our true goal?\nA. $q$-Learning finds a self-consistent $q_{\\theta}$, so that $q_{\\theta}(s_t, a_t) = r_{t+1} + \\gamma  q_{\\theta}(s_{t+1}, \\argmax_{a'} q_{\\theta}(s_{t+1}, a'))$. Our true goal is a policy with maximum expected return. $q$-Learning could converge on a local minimum.","tags":[],"target":[{"selector":[{"type":"RangeSelector","startContainer":"/div[1]/p[51]","startOffset":0,"endContainer":"/div[1]/p[51]","endOffset":178},{"type":"TextPositionSelector","start":11008,"end":11186},{"type":"TextQuoteSelector","exact":"Instead, the Q-Learning update rule trains @import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')qθ(s,a)q_\\theta(s, a)qθ​(s,a)﻿ to be ‘self-consistent’.","prefix":"ver, it’s how we test our agent.","suffix":" By this, we mean that our objec"}]}]}]]